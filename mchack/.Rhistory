train <-  read.csv("train_ajEneEa.csv")
test <- read.csv("test_v2akXPA.csv")
library(data.table)
library(Matrix)
library(dplyr)
library(MLmetrics)
library(lightgbm)
needs(MLmetrics)
needs(lightgbm)
needs(updateR)
needs(installr)
updateR()
View(train)
library(data.table)
library(Matrix)
library(dplyr)
needs(MLmetrics)
needs(lightgbm)
set.seed(257)
library(data.table)
library(Matrix)
library(dplyr)
needs(MLmetrics)
needs(lightgbm)
install.packages("needs")
library(needs)
needs(MLmetrics)
needs(lightgbm)
install.packages(lightgbm)
needs(yaml)
install.packages(lightgbm)
install.packages("lightgbm")
needs(git)
needs("CMake")
needs("git")
git clone --recursive https://github.com/Microsoft/LightGBM
cd LightGBM/R-package
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
library(devtools)
install_github("Microsoft/LightGBM", subdir = "R-package")
library(lightgbm)
install_github("Microsoft/LightGBM", subdir = "R-package")
needs(httpuv)
needs(rgdal)
needs(geojsonio)
needs(rgeos)
install_github("Microsoft/LightGBM", subdir = "R-package")
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package",force=T)
library(lightgbm)
set.seed(257)
train = fread("train_ajEneEa.csv") %>% as.data.frame()
test  = fread("test_v2akXPA.csv")  %>% as.data.frame()
library(fread)
??fread
library(readr)
library(data.table)
library(Matrix)
library(dplyr)
needs(MLmetrics)
library(lightgbm)
needs(dplyr)
library(data.table)
library(Matrix)
needs(dplyr)
needs(MLmetrics)
library(lightgbm)
set.seed(257)
??fread
library(readr)
train = fread("train_ajEneEa.csv") %>% as.data.frame()
test  = fread("test_v2akXPA.csv")  %>% as.data.frame()
View(train)
length(is.na(train))
sapply(train,length(unique()))
for( i in 1:ncol(train)){
print(length(is.na(train[,i])))
}
print(sum(is.na(train[,i])))
for( i in 1:ncol(train)){
print(sum(is.na(train[,i])))
}
View(train)
for( i in 1:ncol(train)){
cat("The missing number of ",i,"variable.")
print(sum(is.na(train[,i])))
}
#missing impute
train$bmi <- train$bmi %>% mutate(~ifelse(is.na(.), median(.,na.rm=T),.))
#missing impute
train$bmi <- train$bmi %>% mutate_all(~ifelse(is.na(.), median(.,na.rm=T),.))
#missing impute
library(Hmisc)
needs(stringi)
#missing impute
library(Hmisc)
impute(train$bmi, median)
train$bmi<-impute(train$bmi, median)
for( i in 1:ncol(train)){
cat("The missing number of ",i,"variable.")
print(sum(is.na(train[,i])))
}
train$smoking_status[which(train$smoking_status=='')]<-"unknown"
View(train)
train1<-spread(train,smoking_status)
needs(dplyr)
train1<-spread(train,smoking_status)
??spread
needs(tidyr)
train1<-spread(train,smoking_status)
train$smoking_status_value<-1
train1<-spread(train,smoking_status,train$smoking_status_value)
train1<-spread(train,smoking_status,smoking_status_value)
View(train1)
train1[is.na(train1)]<-0
train1<-train1[,c(1:10,12:15,11)]
train1$bmi<-as.numeric(train1$bmi)
View(train1)
train1[,c(2,6:8)]<-as.factor(train1[,c(2,6:8)])
train1[,c(2,6:8)]<-sapply(train1[,c(2,6:8)],sa.factor)
train1[,c(2,6:8)]<-sapply(train1[,c(2,6:8)],as.factor)
View(train1)
unique(train1$worktype)
unique(train1$worktype)
unique(train1$work_type)
#Model
dtrain<-
lgb.grid = list(objective = "binary",
metric = "auc",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 100,
max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
min_data_in_bin=100,
min_gain_to_split = 10,
min_data_in_leaf = 30,
is_unbalance = TRUE)
lgb.grid = list(objective = "binary",
metric = "auc",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 100,
max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
min_data_in_bin=100,
min_gain_to_split = 10,
min_data_in_leaf = 30,
is_unbalance = TRUE)
#Model
lgb.train = lgb.Dataset(data=train1[,-1],label=train1[,col(train1)])
# Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb.train, learning_rate = 0.02, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = categoricals.vec, nfold = 5, stratified = TRUE)
categoricals.vec = colnames(train1)[c(grep("cat",colnames(train)))]
c
lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb.train, learning_rate = 0.02, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = categoricals.vec, nfold = 5, stratified = TRUE)
# Train final model
lgb.model = lgb.train(params = lgb.grid, data = lgb.train, learning_rate = 0.02,
num_leaves = 25, num_threads = 2 , nrounds = best.iter,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = categoricals.vec)
# Train final model
lgb.model = lgb.train(params = lgb.grid, data = lgb.train, learning_rate = 0.02,
num_leaves = 25, num_threads = 2 , nrounds = 525,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = categoricals.vec)
cv_folds <- KFold(train1$stroke, nfolds = 5,
stratified = TRUE, seed = 0)
needs(rBayesianOptimization)
cv_folds <- KFold(train1$stroke, nfolds = 5,
stratified = TRUE, seed = 0)
lgb_cv_bayes <- function(num_leaves, learning_rate) {
cv <- lgb.cv(params = list(num_leaves = num_leaves,
learning_rate = learning_rate),
device = 'cpu',
objective = "regression",
metric = "l2",
data = dtrain,
nrounds = 100,
folds = cv_folds,
early_stopping_rounds = 5,
verbose = 0)
list(Score = min(unlist(cv$record_evals$valid$l2$eval)))
}
OPT_Res <- BayesianOptimization(lgb_cv_bayes,
bounds = list(learning_rate = c(0, 1),
num_leaves = c(20L, 40L)),
init_grid_dt = NULL,
init_points = 10,
n_iter = 20,
acq = "ucb",
kappa = 2.576,
eps = 0.0,
verbose = TRUE)
dtrain = lgb.Dataset(data=train1[,-1],label=train1[,col(train1)])
OPT_Res <- BayesianOptimization(lgb_cv_bayes,
bounds = list(learning_rate = c(0, 1),
num_leaves = c(20L, 40L)),
init_grid_dt = NULL,
init_points = 10,
n_iter = 20,
acq = "ucb",
kappa = 2.576,
eps = 0.0,
verbose = TRUE)
varnames = setdiff(colnames(train), c("id", "target"))
varnames = setdiff(colnames(train1), c("id", "target"))
train_sparse = Matrix(as.matrix(data[!is.na(target), varnames, with=F]), sparse=TRUE)
train_sparse = Matrix(as.matrix(train1[!is.na(target), varnames, with=F]), sparse=TRUE)
train_sparse = Matrix(as.matrix(train1[!is.na(target), varnames]), sparse=TRUE)
train_sparse = Matrix(as.matrix(train1[!is.na(stroke), varnames]), sparse=TRUE)
train_sparse = Matrix(train1), sparse=TRUE)
train_sparse = Matrix(train1)
train_sparse = as.matrix(train1)
View(train_sparse)
train1<-train_sparse
dtrain = lgb.Dataset(data=train1[,-1],label=train1[,col(train1)])
dtrain = lgb.Dataset(data=train1[,-1],label=train1[,col(train1)])
vernames
varnames = setdiff(colnames(train1), c("id", "target"))
vernames
varnames
train1<-spread(train,smoking_status,smoking_status_value)
unique(train1$work_type)
train1[is.na(train1)]<-0
train1<-train1[,c(1:10,12:15,11)]
train1$bmi<-as.numeric(train1$bmi)
train1<-data
data<-train1
train1<-spread(train,smoking_status,smoking_status_value)
unique(train1$work_type)
train1[is.na(train1)]<-0
train1<-train1[,c(1:10,12:15,11)]
train1$bmi<-as.numeric(train1$bmi)
data<-train1
varnames
varnames = setdiff(colnames(train1), c("id", "target"))
varnames
Matrix(as.matrix(data[!is.na(stroke), varnames, with=F]), sparse=TRUE)
Matrix(as.matrix(data[!is.na(stroke), varnames]), sparse=TRUE)
Matrix(as.matrix(data[!is.na(data$stroke), varnames, with=F]), sparse=TRUE)
Matrix(as.matrix(data[!is.na(data$stroke), varnames]), sparse=TRUE)
Matrix(as.matrix(data[!is.na(data), varnames]), sparse=TRUE)
data(agaricus.train,package='lightgbm')
library(Matrix)
train2<-data.matrix(train1)
View(train2)
train1$gender<-ifelse(train1$gender=='Male',1,0)
train1$ever_married<-ifelse(train1$ever_married=='Yes',1,0)
train1$Residence_type<-ifelse(train1$Residence_type=='Urban',1,0)
train$work_type_value<-1
train2<-spread(train,work_type,work_type_value)
train2[is.na(train1)]<-0
train2[is.na(train2)]<-0
View(train2)
train2<-spread(train1,work_type,work_type_value)
View(train1)
train1$work_type_value<-1
train2<-spread(train1,work_type,work_type_value)
train2[is.na(train2)]<-0
train2<-train2[,c(1:13,15:19,14)]
train3<-data.matrix(train2)
train3<-Matrix(train3, sparse=TRUE)
#Model
lgb.train = lgb.Dataset(data=train3[,-1],label=train3[,col(train3)])
dtrain = lgb.Dataset(data=train3[,-1],label=train3[,col(train3)])
train_label<=train2[,19]
train_label<-train2[,19]
train_data<-train2[,-19]
train3<-data.matrix(train_data)
train3<-Matrix(train_data, sparse=TRUE)
train3<-Matrix(train3, sparse=TRUE)
dtrain = lgb.Dataset(data=train3,label=train_label)
cv_folds <- KFold(train1$stroke, nfolds = 5,
stratified = TRUE, seed = 0)
lgb_cv_bayes <- function(num_leaves, learning_rate) {
cv <- lgb.cv(params = list(num_leaves = num_leaves,
learning_rate = learning_rate),
device = 'cpu',
objective = "regression",
metric = "l2",
data = dtrain,
nrounds = 100,
folds = cv_folds,
early_stopping_rounds = 5,
verbose = 0)
list(Score = min(unlist(cv$record_evals$valid$l2$eval)))
}
OPT_Res <- BayesianOptimization(lgb_cv_bayes,
bounds = list(learning_rate = c(0, 1),
num_leaves = c(20L, 40L)),
init_grid_dt = NULL,
init_points = 10,
n_iter = 20,
acq = "ucb",
kappa = 2.576,
eps = 0.0,
verbose = TRUE)
model <- lgb.cv(OPT_Res, dtrain, 10, nfold=5, min_data=1,  early_stopping_rounds=10)
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=0.9883, early_stopping_rounds=10)
model <- lgb.cv(lgb.grid, dtrain, 10, nfold=5, min_data=1, learning_rate=0.9883, early_stopping_rounds=10)
model <- lgb.cv(lgb.grid, dtrain, 10, nfold=5, min_data=1, learning_rate=0.9883, early_stopping_rounds=50)
model <- lgb.cv(lgb.grid, dtrain, 50, nfold=5, min_data=1, learning_rate=0.9883, early_stopping_rounds=10)
#Model
lgb.train = lgb.Dataset(data=train3,label=train_label)
categoricals.vec = colnames(train1)[c(grep("cat",colnames(train)))]
#parameters
lgb.grid = list(objective = "binary",
metric = "auc",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 100,
max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
min_data_in_bin=100,
min_gain_to_split = 10,
min_data_in_leaf = 30,
is_unbalance = TRUE)
# Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb.train, learning_rate = 0.02, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = categoricals.vec, nfold = 5, stratified = TRUE)
best.iter = lgb.model.cv$best_iter
# Train final model
lgb.model = lgb.train(params = lgb.grid, data = lgb.train, learning_rate = 0.02,
num_leaves = 25, num_threads = 2 , nrounds = best.iter,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = categoricals.vec)
test$bmi<-impute(test$bmi, median)
test$smoking_status[which(test$smoking_status=='')]<-"unknown"
test$smoking_status_value<-1
test<-spread(test,smoking_status,smoking_status_value)
test[is.na(test)]<-0
test$bmi<-as.numeric(test$bmi)
test$gender<-ifelse(test$gender=='Male',1,0)
test$ever_married<-ifelse(test$ever_married=='Yes',1,0)
test$Residence_type<-ifelse(test$Residence_type=='Urban',1,0)
test$work_type_value<-1
test<-spread(test,work_type,work_type_value)
test[is.na(test)]<-0
test<-data.matrix(test)
test<-Matrix(test, sparse=TRUE)
preds =predict(lgb.model,test)
preds
length(preds)
preds =data.frame(test$id,predict(lgb.model,test))
preds =data.frame(test[,1],predict(lgb.model,test))
View(preds)
colnames(preds)<-c("id","stroke")
fwrite(preds, "submission.csv")
